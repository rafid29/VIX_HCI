{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "database.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xALvkUyXLyrc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8df7d59e-ed08-4fbf-ddb9-11d262e1fcad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn import over_sampling\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from scipy.stats import uniform\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "main = pd.read_csv('/content/drive/My Drive/data/application_train.csv')\n",
        "main\n",
        "main['SK_ID_CURR'].nunique()\n",
        "test = pd.read_csv('/content/drive/My Drive/data/application_test.csv')\n",
        "test\n",
        "test['SK_ID_CURR'].nunique()\n",
        "\n",
        "prev = pd.read_csv('/content/drive/My Drive/data/previous_application.csv')\n",
        "prev\n",
        "\n",
        "bureau = pd.read_csv('/content/drive/My Drive/data/bureau.csv')\n",
        "bureau\n",
        "\n",
        "prev_app = prev.groupby(['SK_ID_CURR'])['SK_ID_CURR'].agg(['count']).reset_index()\n",
        "prev_app.columns = ['SK_ID_CURR','TOTAL_PREV_APP']\n",
        "prev_app\n",
        "\n",
        "bureau_cred = bureau.groupby(['SK_ID_CURR'])['SK_ID_CURR'].agg(['count']).reset_index()\n",
        "bureau_cred.columns = ['SK_ID_CURR','TOTAL_BUREAU_LOAN']\n",
        "bureau_cred\n",
        "\n",
        "df = (main.merge(prev_app, how='right', on='SK_ID_CURR')).merge(bureau_cred, how='left', on='SK_ID_CURR')\n",
        "df\n",
        "\n",
        "#data clearning\n",
        "#duplicat\n",
        "df.duplicated().sum()\n",
        "\n",
        "#missing data\n",
        "total = df.isnull().sum().sort_values(ascending = False)\n",
        "percent = (df.isnull().sum()/df.isnull().count()*100).sort_values(ascending = False)\n",
        "missing_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent']).reset_index()\n",
        "missing_data.loc[missing_data['Percent'] >60]\n",
        "\n",
        "(missing_data.loc[missing_data['Percent'] >0]).shape\n",
        "#feature target\n",
        "df['TARGET'].value_counts()\n",
        "df['TARGET'].value_counts(normalize=True).plot.bar(figsize=(4,4), title= 'Distribusi Tareget', color=['black','yellow'])\n",
        "plt.show()\n",
        "\n",
        "plt.figure(1)\n",
        "plt.subplot(221)\n",
        "df['CODE_GENDER'].value_counts(normalize=True).plot.bar(figsize=(8,8), title= 'Client Gender', color=['black','yellow'])\n",
        "plt.subplot(222)\n",
        "\n",
        "df['FLAG_OWN_CAR'].value_counts(normalize=True).plot.bar(title= 'Apa client memiliki mobil?', color=['black','yellow'])\n",
        "plt.subplot(223)\n",
        "\n",
        "df['CNT_CHILDREN'].value_counts(normalize=True).plot.bar(title= 'Berapa banyak client yang memiliki anak?', color=['black','yellow','blue','red'])\n",
        "plt.subplot(224)\n",
        "\n",
        "df['FLAG_OWN_REALTY'].value_counts(normalize=True).plot.bar(figsize=(8,8), title= 'apa client memiliki Realty?', color=['black','yellow'])\n",
        "\n",
        "plt.show()\n",
        "\n",
        "df['CODE_GENDER'].value_counts()\n",
        "\n",
        "df['CNT_CHILDREN'].value_counts()\n",
        "\n",
        "df['FLAG_OWN_CAR'].value_counts()\n",
        "\n",
        "df['FLAG_OWN_REALTY'].value_counts()\n",
        "\n",
        "#Data Preprocessing Data Training\n",
        "df = (main.merge(prev_app, how='left', on='SK_ID_CURR')).merge(bureau_cred, how='left', on='SK_ID_CURR')\n",
        "df\n",
        "#Converting DAYS_BIRTH to years to get client age\n",
        "df['AGE'] = df['DAYS_BIRTH']/-365\n",
        "df.drop(columns='DAYS_BIRTH', inplace=True)\n",
        "data_age = df[['TARGET', 'AGE']]\n",
        "data_age['AGE_GROUP'] = pd.cut(data_age['AGE'], bins = np.linspace(20, 70, num=6))\n",
        "data_age = (data_age.groupby(['AGE_GROUP']).mean()).sort_values('TARGET')\n",
        "data_age\n",
        "plt.barh(data_age.index.astype(str), round(100*data_age['TARGET']), color='indianred')\n",
        "\n",
        "plt.ylabel('Age Group (years)')\n",
        "plt.xlabel('Failure to Repay (%)')\n",
        "plt.title('Failure to Repay by Age Group');\n",
        "plt.show()\n",
        "\n",
        "#Converting DAYS_EMPLOYED to years\n",
        "df['YEARS_EMPLOYED'] = df['DAYS_EMPLOYED']/-365\n",
        "df.drop(columns='DAYS_EMPLOYED', inplace=True)\n",
        "\n",
        "df['YEARS_EMPLOYED'].describe()\n",
        "\n",
        "\n",
        "\n",
        "df['YEARS_EMPLOYED'].replace({df['YEARS_EMPLOYED'].min(): np.nan}, inplace=True)\n",
        "df['YEARS_EMPLOYED'].plot.hist(title = 'Years Employment Histogram');\n",
        "plt.xlabel('Years Employment');\n",
        "\n",
        "data_employed = df[['TARGET', 'YEARS_EMPLOYED','AGE']]\n",
        "data_employed['YEARS_EMPLOYED_GROUP'] = pd.cut(data_employed['YEARS_EMPLOYED'], bins = np.linspace(0, 50, num=6))\n",
        "data_employed = ((data_employed.groupby(['YEARS_EMPLOYED_GROUP']).mean())).sort_values('TARGET')\n",
        "data_employed\n",
        "\n",
        "\n",
        "\n",
        "plt.barh(data_employed.index.astype(str), round(100*data_employed['TARGET']), color='indianred')\n",
        "\n",
        "plt.ylabel('Years Employed Group (years)')\n",
        "plt.xlabel('Failure to Repay (%)')\n",
        "plt.title('Failure to Repay by Years Employed Group');\n",
        "plt.show()\n",
        "\n",
        "plt.barh(data_employed.index.astype(str), round(data_employed['AGE']), color='indianred')\n",
        "\n",
        "plt.ylabel('Years Employed Group (years)')\n",
        "plt.xlabel('Average Age')\n",
        "plt.title('Average Age vs Years Employed Group');\n",
        "plt.show()\n",
        "\n",
        "#Converting DAYS_REGISTRATION to years\n",
        "df['YEARS_REGISTRATION'] = df['DAYS_REGISTRATION']/-365\n",
        "df.drop(columns='DAYS_REGISTRATION', inplace=True)\n",
        "\n",
        "df['YEARS_REGISTRATION'].describe()\n",
        "\n",
        "data_regist = df[['TARGET', 'YEARS_REGISTRATION']]\n",
        "data_regist['YEARS_REGISTRATION_GROUP'] = pd.cut(data_regist['YEARS_REGISTRATION'], bins = np.linspace(0, 70, num=8))\n",
        "data_regist = (data_regist.groupby(['YEARS_REGISTRATION_GROUP']).mean())\n",
        "data_regist\n",
        "\n",
        "plt.barh(data_regist.index.astype(str), round(100*data_regist['TARGET']), color='indianred')\n",
        "\n",
        "plt.ylabel('Years Registration Group (years)')\n",
        "plt.xlabel('Failure to Repay (%)')\n",
        "plt.title('Failure to Repay by Years Registration Group');\n",
        "plt.show()\n",
        "\n",
        "#Converting DAYS_ID_PUBLISH to years\n",
        "df['YEARS_ID_PUBLISH'] = df['DAYS_ID_PUBLISH']/-365\n",
        "df.drop(columns='DAYS_ID_PUBLISH', inplace=True)\n",
        "\n",
        "df['YEARS_ID_PUBLISH'].describe()\n",
        "data_id_publish = df[['TARGET', 'YEARS_ID_PUBLISH']]\n",
        "\n",
        "data_id_publish['YEARS_ID_PUBLISH_GROUP'] = pd.cut(data_id_publish['YEARS_ID_PUBLISH'], bins = np.linspace(0, 20, num=5))\n",
        "data_id_publish = (data_id_publish.groupby(['YEARS_ID_PUBLISH_GROUP']).mean()).sort_values('TARGET')\n",
        "data_id_publish\n",
        "\n",
        "plt.barh(data_id_publish.index.astype(str), round(100*data_id_publish['TARGET']), color='indianred')\n",
        "\n",
        "plt.ylabel('Change Document Identity Before Application (years)')\n",
        "plt.xlabel('Failure to Repay (%)')\n",
        "plt.title('Failure to Repay by DAYS_ID_PUBLISH');\n",
        "plt.show()\n",
        "\n",
        "#Replace XNA values with NaN\n",
        "for col in df.select_dtypes(include = [\"object\"]).columns:\n",
        "    print(f'''Value count kolom {col}:''')\n",
        "    print(df[col].value_counts())\n",
        "    print()\n",
        "\n",
        "df.CODE_GENDER.replace(\"XNA\", np.nan, inplace=True)\n",
        "df.ORGANIZATION_TYPE.replace(\"XNA\", np.nan, inplace=True)\n",
        "df.shape\n",
        "\n",
        "total = df.isnull().sum().sort_values(ascending = False)\n",
        "percent = (df.isnull().sum()/df.isnull().count()*100).sort_values(ascending = False)\n",
        "missing_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent']).reset_index()\n",
        "missing_data.loc[missing_data['Percent'] >60]\n",
        "\n",
        "df.drop(columns=list(missing_data['index'].loc[missing_data['Percent'] >60]),inplace=True)\n",
        "df.shape\n",
        "\n",
        "pd.set_option(\"max_columns\", None)\n",
        "df.head()\n",
        "\n",
        "#Missing Value Imputation\n",
        "null = df.isnull().sum().reset_index()\n",
        "null_table = null.loc[null[0] > 0]\n",
        "null_table.shape\n",
        "\n",
        "pd.set_option(\"max_columns\", None)\n",
        "data_null = df[null_table['index'].tolist()]\n",
        "data_null\n",
        "\n",
        "data_null.select_dtypes(exclude = [\"object\"] ).shape[1]\n",
        "data_null.select_dtypes(include = [\"object\"] ).shape[1]\n",
        "\n",
        "def miss_numerical(df):\n",
        "    \n",
        "    numerical_features = df.select_dtypes(exclude = [\"object\"] ).columns\n",
        "    for f in numerical_features:\n",
        "        df[f] = df[f].fillna(df[f].median())\n",
        "    return df\n",
        "\n",
        "def miss_categorical(df):\n",
        "    \n",
        "    categorical_features = df.select_dtypes(include = [\"object\"]).columns\n",
        "    for f in categorical_features:\n",
        "        df[f] = df[f].fillna(df[f].mode()[0])\n",
        "    return df\n",
        "\n",
        "def transform_feature(df):\n",
        "    df = miss_numerical(df)\n",
        "    df = miss_categorical(df)\n",
        "    return df\n",
        "\n",
        "df = transform_feature(df)\n",
        "pd.set_option(\"max_columns\", None)\n",
        "df\n",
        "\n",
        "null = df.isnull().sum().reset_index()\n",
        "null_table = null.loc[null[0] > 0]\n",
        "null_table.shape[0]\n",
        "\n",
        "def encoder(df):\n",
        "    scaler = MinMaxScaler()\n",
        "    numerical = df.select_dtypes(exclude = [\"object\"]).columns\n",
        "    numerical = numerical[1:]\n",
        "    features_transform = pd.DataFrame(data=df)\n",
        "    features_transform[numerical] = scaler.fit_transform(df[numerical])\n",
        "    display(features_transform.head(n = 5))\n",
        "    return df\n",
        "\n",
        "df = encoder(df)\n",
        "df.shape\n",
        "\n",
        "df[df.select_dtypes(include = [\"object\"]).columns].head()\n",
        "for col in df.select_dtypes(include = [\"object\"]).columns:\n",
        "    print(f'''Value count kolom {col}:''')\n",
        "    print(df[col].value_counts())\n",
        "    print()\n",
        "\n",
        "le = LabelEncoder()\n",
        "le_count = 0\n",
        "\n",
        "for col in df:\n",
        "    if df[col].dtype == 'object':\n",
        "        if len(list(df[col].unique())) <= 2:\n",
        "            le.fit(df[col])\n",
        "            df[col] = le.transform(df[col])            \n",
        "            le_count += 1\n",
        "\n",
        "           \n",
        "print('%d columns were label encoded.' % le_count)\n",
        "df.shape\n",
        "le_count = 0\n",
        "\n",
        "for col in df:\n",
        "    if df[col].dtype == 'object':\n",
        "        if len(list(df[col].unique())) > 2:\n",
        "            onehots = pd.get_dummies(df[col])\n",
        "            df = df.join(onehots)\n",
        "            df.drop(columns=col, inplace=True)\n",
        "            le_count += 1\n",
        "           \n",
        "print('%d columns were one hot encoded.' % le_count)\n",
        "df.shape\n",
        "df.head()\n",
        "\n",
        "df_train = df.iloc[:,1:]\n",
        "DF_TRAIN = df_train\n",
        "DF_TRAIN\n",
        "\n",
        "df_test = df.iloc[:,1:]\n",
        "df_test\n",
        "DF_TEST = df_test\n",
        "DF_TEST\n",
        "\n",
        "target = df_train['TARGET']\n",
        "\n",
        "df_train, df_test = df_train.align(df_test, join = 'inner', axis = 1)\n",
        "df_train['TARGET'] = target\n",
        "\n",
        "def encoder(df):\n",
        "    scaler = MinMaxScaler()\n",
        "    numerical = df.select_dtypes(exclude = [\"object\"]).columns\n",
        "    numerical1 = numerical[2:4]\n",
        "    numerical2 = numerical[5:]\n",
        "    numerical_all = numerical1 | numerical2\n",
        "    features_transform = pd.DataFrame(data=df)\n",
        "    features_transform[numerical_all] = scaler.fit_transform(df[numerical_all])\n",
        "    display(features_transform.head(n = 5))\n",
        "    return df\n",
        "\n",
        "df = encoder(df)\n",
        "scaler_AMT_CREDIT = MinMaxScaler()\n",
        "scaler_AMT_CREDIT.fit(df['AMT_CREDIT'].values.reshape(len(df), 1))\n",
        "\n",
        "df['AMT_CREDIT'] = scaler_AMT_CREDIT.transform(df['AMT_CREDIT'].values.reshape(len(df), 1))\n",
        "df.head()\n",
        "#Modeling\n",
        "X = df_train.drop(columns = ['TARGET'])\n",
        "Y = df_train[['TARGET']]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size = 0.2,random_state = 42)\n",
        "X_train.shape\n",
        "X_test.shape\n",
        "y_train.shape\n",
        "y_test.shape\n",
        "y_train['TARGET'].value_counts()\n",
        "x = X_train[[col for col in X_train.columns if (str(X_train[col].dtype) != 'object')]]\n",
        "y = y_train['TARGET'].values\n",
        "print(x.shape)\n",
        "print(y.shape)\n",
        "x_over_SMOTE, y_over_SMOTE = over_sampling.SMOTE(0.5).fit_resample(x, y)\n",
        "print('SMOTE')\n",
        "print(pd.Series(y_over_SMOTE).value_counts())\n",
        "\n",
        "data_train = x_over_SMOTE\n",
        "data_train['TARGET'] = pd.DataFrame(y_over_SMOTE)\n",
        "\n",
        "print('Original')\n",
        "print(pd.Series(y).value_counts())\n",
        "print('')\n",
        "print('SMOTE')\n",
        "print(pd.Series(y_over_SMOTE).value_counts())\n",
        "X_train = data_train.drop(columns = ['TARGET'])\n",
        "y_train = data_train[['TARGET']]\n",
        "\n",
        "def eval_classification(model, pred, xtrain, ytrain, xtest, ytest):\n",
        "    print(\"Accuracy (Test Set): %.2f\" % accuracy_score(ytest, pred))\n",
        "    print(\"Precision (Test Set): %.2f\" % precision_score(ytest, pred))\n",
        "    print(\"Recall (Test Set): %.2f\" % recall_score(ytest, pred))\n",
        "    print(\"F1-Score (Test Set): %.2f\" % f1_score(ytest, pred))\n",
        "    \n",
        "    fpr, tpr, thresholds = roc_curve(ytest, pred, pos_label=1) # pos_label: label yang kita anggap positive\n",
        "    print(\"AUC: %.2f\" % auc(fpr, tpr))\n",
        "\n",
        "#Logistic Regression\n",
        "logres = LogisticRegression(random_state=42)\n",
        "logres.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred_proba_logres = logres.predict_proba(X_test)\n",
        "y_pred_test_logres = logres.predict(X_test)\n",
        "\n",
        "eval_classification(logres, y_pred_test_logres, X_train, y_train, X_test, y_test)\n",
        "\n",
        "Log_ROC_auc = roc_auc_score(y_test, y_pred_test_logres)\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_logres [:, 1])\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label = \"Logistik Regression Model (area = %0.2f)\" % Log_ROC_auc)\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "feat_importances = pd.Series(logres.coef_[0], index=X.columns)\n",
        "ax = feat_importances.nlargest(32).plot(kind='barh', figsize=(10, 8))\n",
        "ax.invert_yaxis()\n",
        "\n",
        "plt.xlabel('Score')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Feature Importance Score')\n",
        "plt.show()\n",
        "\n",
        "feat_importances.abs().sort_values(ascending = False)\n",
        "\n",
        "cf = confusion_matrix(y_test, y_pred_test_logres)\n",
        "cf\n",
        "y_pred_train_logres = logres.predict(X_train)\n",
        "y_pred_test_logres = logres.predict(X_test)\n",
        "print(\"Precision (Train Set): \" +str(precision_score(y_train, y_pred_train_logres)))\n",
        "print(\"Precision (Test Set):\" +str(precision_score(y_test, y_pred_test_logres)))\n",
        "\n",
        "#Hyperparameter Tuning\n",
        "penalty = ['l2','l1','elasticnet']\n",
        "C = [0.0001, 0.005] \n",
        "hyperparameters = dict(penalty=penalty, C=C)\n",
        "\n",
        "logres = LogisticRegression(random_state=42) # Init Logres dengan Gridsearch, cross validation = 5\n",
        "logres_tuning = RandomizedSearchCV(logres, hyperparameters, cv=5, random_state=42, scoring='roc_auc')\n",
        "logres_tuning.fit(X_train, y_train)\n",
        "\n",
        "y_pred_proba_logres_tuning = logres_tuning.predict_proba(X_test)\n",
        "y_pred_test_logres_tuning = logres_tuning.predict(X_test)\n",
        "eval_classification(logres_tuning, y_pred_test_logres_tuning, X_train, y_train, X_test, y_test)\n",
        "Log_ROC_auc = roc_auc_score(y_test, y_pred_test_logres_tuning)\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_logres_tuning [:, 1])\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label = \"Logistic Regression Tuned Model (area = %0.2f)\" % Log_ROC_auc)\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "print('Best algorithm:', logres_tuning.best_estimator_.get_params()['penalty'])\n",
        "print('Best C:', logres_tuning.best_estimator_.get_params()['C'])\n",
        "cf = confusion_matrix(y_test, y_pred_test_logres_tuning)\n",
        "cf\n",
        "y_pred_train = logres_tuning.predict(X_train)\n",
        "y_pred_test = logres_tuning.predict(X_test)\n",
        "print(\"Precision (Train Set): \" +str(precision_score(y_train, y_pred_train)))\n",
        "print(\"Precision (Test Set):\" +str(precision_score(y_test, y_pred_test)))\n",
        "\n",
        "xg = XGBClassifier(random_state=42)\n",
        "xg.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred_proba_xg = xg.predict_proba(X_test)\n",
        "y_pred_test_xg = xg.predict(X_test)\n",
        "eval_classification(xg, y_pred_test_xg, X_train, y_train, X_test, y_test)\n",
        "Log_ROC_auc = roc_auc_score(y_test, y_pred_test_xg)\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_xg [:, 1])\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label = \"XGBoost Model (area = %0.2f)\" % Log_ROC_auc)\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "cf = confusion_matrix(y_test, y_pred_test_xg)\n",
        "cf\n",
        "\n",
        "DF_TRAIN.shape\n",
        "DF_TRAIN.head()\n",
        "\n",
        "DF_TRAIN['AMT_CREDIT'] = scaler_AMT_CREDIT.inverse_transform(DF_TRAIN['AMT_CREDIT'].values.reshape(len(DF_TRAIN), 1))\n",
        "DF_TRAIN.head()\n",
        "X = DF_TRAIN.drop(columns = ['TARGET'])\n",
        "Y = DF_TRAIN[['TARGET']]\n",
        "XTrain, XTest, yTrain, yTest = train_test_split(X,Y,test_size = 0.2,random_state = 42)\n",
        "DF_TRAIN.shape\n",
        "y_pred_test_logres.shape\n",
        "XTest.shape\n",
        "XTest['TARGET_PRED_RESULT'] = y_pred_test_logres\n",
        "XTest['TARGET'] = yTest\n",
        "XTest.shape\n",
        "XTest.head()\n",
        "#sebelum\n",
        "Defaulters = XTest.groupby(['TARGET'])['AMT_CREDIT'].sum().reset_index()\n",
        "Defaulters\n",
        "Total_Defaulters = XTest.groupby(['TARGET'])['AMT_CREDIT'].count().reset_index()\n",
        "Total_Defaulters\n",
        "LGDBefore = Defaulters['AMT_CREDIT'].loc[Defaulters['TARGET'] == 1]\n",
        "LGDBefore\n",
        "#sesudah\n",
        "FalseNegative = (XTest.loc[(XTest['TARGET'] == 1) & (XTest['TARGET_PRED_RESULT'] == 0)])\n",
        "FalseNegative.shape\n",
        "LGDAfter = FalseNegative['AMT_CREDIT'].sum()\n",
        "LGDAfter\n",
        "LGDAfter - LGDBefore\n",
        "LGD_Decreased_Percentage = ((LGDAfter-LGDBefore)/LGDBefore)*100\n",
        "LGD_Decreased_Percentage\n",
        "\n",
        "logres = LogisticRegression(random_state=42)\n",
        "logres.fit(X_train, y_train)\n",
        "y_pred_proba_logres_df_test = logres.predict_proba(df_test)\n",
        "y_pred_proba_logres_df_test\n",
        "df_test.shape\n",
        "test.shape\n",
        "PredictResultTest = test['SK_ID_CURR'].reset_index()\n",
        "PredictResultTest['TARGET'] = round(pd.DataFrame(y_pred_proba_logres_df_test[:,1]),1)\n",
        "PredictResultTest.drop('index', axis=1, inplace=True)\n",
        "PredictResultTest.head()\n",
        "PredictResultTest.shape\n",
        "PredictResultTest.to_csv(\"y_pred_proba_df_test.txt\", sep=',', header=True, index=False) "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "count    307511.000000\n",
        "mean         13.660604\n",
        "std           9.651743\n",
        "min          -0.000000\n",
        "25%           5.506849\n",
        "50%          12.339726\n",
        "75%          20.491781\n",
        "max          67.594521\n",
        "Name: YEARS_REGISTRATION, dtype: float64"
      ],
      "metadata": {
        "id": "ijwXVpO1PpdQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "count    307511.000000\n",
        "\n",
        "mean       -174.835742\n",
        "\n",
        "std         387.056895\n",
        "\n",
        "min       -1000.665753\n",
        "\n",
        "25%           0.791781\n",
        "\n",
        "50%           3.323288\n",
        "\n",
        "75%           7.561644\n",
        "\n",
        "max          49.073973\n",
        "\n",
        "Name: YEARS_EMPLOYED, dtype: float64"
      ],
      "metadata": {
        "id": "B5f5HwJLJjoI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AGE_GROUP,TARGET,AGE\n",
        "\n",
        "\"(60.0, 70.0]\",0.049214424239017396,63.35593344850568\n",
        "\n",
        "\"(50.0, 60.0]\",0.061297052687255736,54.97657466218351\n",
        "\n",
        "\"(40.0, 50.0]\",0.07650801531205498,44.75607944385792\n",
        "\n",
        "\"(30.0, 40.0]\",0.09583515575642708,35.123196833011455\n",
        "\n",
        "\"(20.0, 30.0]\",0.11456875680238111,26.622040734110993"
      ],
      "metadata": {
        "id": "kc6XfIytIoAH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y    205732\n",
        "\n",
        "N     85325\n",
        "\n",
        "Name: FLAG_OWN_REALTY, dtype: int64"
      ],
      "metadata": {
        "id": "KR4UXwz3GN4h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "F      192765\n",
        "\n",
        "M       98288\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "XNA         4\n",
        "\n",
        "Name: CODE_GENDER, dtype: int64::teks tebal:"
      ],
      "metadata": {
        "id": "asMzElgND8cf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "0.0     202784\n",
        "\n",
        "1.0      58450\n",
        "\n",
        "2.0      25689\n",
        "\n",
        "3.0       3594\n",
        "\n",
        "4.0        418\n",
        "\n",
        "5.0         81\n",
        "\n",
        "6.0         20\n",
        "\n",
        "7.0          7\n",
        "\n",
        "14.0         3\n",
        "\n",
        "8.0          2\n",
        "\n",
        "9.0          2\n",
        "\n",
        "12.0         2\n",
        "\n",
        "10.0         2\n",
        "\n",
        "19.0         2\n",
        "\n",
        "11.0         1\n",
        "\n",
        "Name: CNT_CHILDREN, dtype: int64"
      ],
      "metadata": {
        "id": "4gnxjz1FEefL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "N    192556\n",
        "\n",
        "Y     98501\n",
        "\n",
        "Name: FLAG_OWN_CAR, dtype: int64"
      ],
      "metadata": {
        "id": "ZS5iFVP9E6du"
      }
    }
  ]
}